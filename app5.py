import streamlit as st
import json
import re
import os
import requests
from bs4 import BeautifulSoup
import pdfplumber
import io
import pandas as pd
from collections import defaultdict
from typing import List, Dict, Any
from datetime import datetime

# ==========================================
# 1. è¨­å®šãƒ»å®šæ•°å®šç¾©
# ==========================================
# ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œæ™‚ã¯ãƒ•ã‚¡ã‚¤ãƒ«ãŒä½œæˆã•ã‚Œã¾ã™ãŒã€Streamlit Cloudã§ã¯ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒåˆ‡ã‚Œã‚‹ã¨å‰Šé™¤ã•ã‚Œã¾ã™ã€‚
DATASET_PATH = "incident_dataset.json"
CHECKLISTS_PATH = "generated_checklists.json"

# â˜…â˜…â˜…â˜…â˜… ã“ã“ãŒã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆURLã§ã™ â˜…â˜…â˜…â˜…â˜…
TARGET_URLS = [
    "https://www.med-safe.jp/report/index.html",  # åŒ»ç™‚å®‰å…¨æƒ…å ±ï¼ˆä¸»ã«PDFãƒªãƒ³ã‚¯é›†ï¼‰
    "https://www.med-safe.jp/medical_safety/index.html",  # åŒ»ç™‚äº‹æ•…æƒ…å ±åé›†ç­‰äº‹æ¥­
]

# ä¿®æ­£1: è„³ç¥çµŒå¤–ç§‘ç‰¹æœ‰ã®å‡¦ç½®ã¨ç®¡ç†é …ç›®ã‚’è¿½åŠ 
PROCEDURES = {
    "æ‚£è€…ç¢ºèªãƒ»æŒ‡å°": ["æ‚£è€…", "ç¢ºèª", "æŒ‡å°", "èª¬æ˜", "åŒæ„", "ã‚¢ãƒ¬ãƒ«ã‚®ãƒ¼"],
    "æ¡è¡€": ["æ¡è¡€", "è¡€æ¶²", "é™è„ˆ", "è¡€ç®¡", "ç©¿åˆº"],
    "è¼¸è¡€": ["è¼¸è¡€", "è¡€æ¶²è£½å‰¤", "è¡€æ¶²å‹", "ãƒãƒ³ãƒ”ãƒ³ã‚°"],
    "ç‚¹æ»´ãƒ»è–¬å‰¤": ["ç‚¹æ»´", "è¼¸æ¶²", "IV", "è–¬å‰¤æŠ•ä¸", "ã‚·ãƒªãƒ³ã‚¸ãƒãƒ³ãƒ—", "è¼¸æ¶²ãƒãƒ³ãƒ—", "æŠ—å‡å›ºè–¬", "æŠ—ã¦ã‚“ã‹ã‚“è–¬"],
    "æ‰‹è¡“": ["æ‰‹è¡“", "ã‚ªãƒš", "è¡“ä¸­", "éº»é…”", "åŸ·åˆ€", "ã‚¬ãƒ¼ã‚¼ã‚«ã‚¦ãƒ³ãƒˆ"],
    "å†…è¦–é¡": ["å†…è¦–é¡", "èƒƒã‚«ãƒ¡ãƒ©", "å¤§è…¸", "ã‚¹ã‚³ãƒ¼ãƒ—", "CF", "GF"],
    "æ°—ç®¡æŒ¿ç®¡": ["æŒ¿ç®¡", "æ°—é“", "æ›æ°—", "ãƒãƒ¥ãƒ¼ãƒ–", "æŠœç®¡"],
    "ä¸­å¿ƒé™è„ˆã‚«ãƒ†ãƒ¼ãƒ†ãƒ«": ["CVC", "ä¸­å¿ƒé™è„ˆ", "ã‚«ãƒ†ãƒ¼ãƒ†ãƒ«", "CV", "ã‚¬ã‚¤ãƒ‰ãƒ¯ã‚¤ãƒ¤ãƒ¼"],
    "ãƒ‰ãƒ¬ãƒŠãƒ¼ã‚¸ç®¡ç†": ["ãƒ‰ãƒ¬ãƒŠãƒ¼ã‚¸", "è„³å®¤", "è…°æ¤", "ã‚·ãƒ£ãƒ³ãƒˆ", "é«„æ¶²"],
    "è„³ç¥çµŒå¤–ç§‘ç®¡ç†": ["æ„è­˜ãƒ¬ãƒ™ãƒ«", "ç³å­”", "éº»ç—º", "é ­è“‹å†…åœ§", "ã‚¯ãƒƒã‚·ãƒ³ã‚°"],
}

# ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆé …ç›®æŠ½å‡ºç”¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
ACTION_KEYWORDS = [
    "ç¢ºèª", "ç…§åˆ", "äºŒé‡", "å›ºå®š", "ç·©ã‚ã‚‹", "å®Ÿæ–½", "è¨˜éŒ²", "å¾¹åº•", "ç¶­æŒ", "å¤‰æ›´",
    "æŠœé‡", "é§†è¡€å¸¯", "æ­¢è¡€", "éƒ¨ä½", "é¸æŠ", "ã‚¢ã‚»ã‚¹ãƒ¡ãƒ³ãƒˆ", "æŠŠæ¡", "æŒ‡ç¤º", "éµå®ˆ",
    "è­˜åˆ¥", "æ³¨æ„", "ã‚«ã‚¦ãƒ³ãƒˆ", "æ¸¬å®š", "æ¯”è¼ƒ", "è¦³å¯Ÿ"
]

# ãƒã‚¤ã‚ºé™¤å»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ (å¤‰æ›´ãªã—)
NOISE_KEYWORDS = [
    "å†ç™ºé˜²æ­¢ã«åŠªã‚ã‚‹", "ã”ç†è§£ã„ãŸã ã‘ã‚Œã°å¹¸ã„", "æƒ…å ±æä¾›ã¨ä½ç½®ã¥ã‘", "æ–½è¡Œã•ã‚Œã¦ã„ã‚‹",
    "å†ç™ºé˜²æ­¢ã«å‘ã‘ã¦å–ã‚Šçµ„ã‚“ã§ã„ã‚‹å§¿ã‚’", "å†ç™ºé˜²æ­¢ã«è³‡ã™ã‚‹", "æƒ…å ±æä¾›ã¨ä½ç½®ã¥ã‘ã¦ãŠã‚Šã¾ã™",
    "ãƒ’ãƒ¤ãƒªãƒ»ãƒãƒƒãƒˆäº‹ä¾‹åé›†äº‹æ¥­", "è³‡æ–™ï¼“", "å…¨èˆ¬ã‚³ãƒ¼ãƒ‰åŒ–æƒ…å ±", "è£½é€ ï¼ˆè¼¸å…¥è²©å£²ï¼‰æ¥­è€…å",
    "å®šç‚¹åŒ»ç™‚æ©Ÿé–¢ä¸€è¦§", "å¹³æˆ", "æœˆæ—¥ç¾åœ¨", "å®šç‚¹åŒ»ç™‚æ©Ÿé–¢ã¨ã¯", "äº‹æ•…ã®å†…å®¹åŒ»ç™‚",
    "ç™ºç”Ÿå ´é¢", "äº‹ä¾‹ã®æ¦‚è¦", "å…¨èˆ¬ã‚³ãƒ¼ãƒ‰åŒ–", "åŸå› åˆ†æ", "å†ç™ºé˜²æ­¢ç­–", "å®Ÿæ–½ã—ãŸåŒ»ç™‚è¡Œç‚ºã®ç›®çš„",
    "æ¤œè¨çµæœ", "ç—…é™¢å", "éƒ¨é–€å", "è·ç¨®", "æ€§åˆ¥", "å¹´é½¢", "è³¼å…¥å¹´æœˆ", "1517", "16",
    "ç™ºç”Ÿè¦å› ", "å¯¾å¿œã¨å¯¾ç­–", "çµŒéã¨çµæœ«", "èƒŒæ™¯è¦å› ", "åˆ¥ç´™", "å‚ç…§"
]

# ä¿®æ­£2: è„³ç¥çµŒå¤–ç§‘ç—…æ£Ÿå‘ã‘ã®ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆé …ç›®ã‚’è¿½åŠ ãƒ»æ‹¡å……
STANDARD_CHECKLIST_ITEMS: Dict[str, List[str]] = {
    # æ—¢å­˜ã®é …ç›® (ä¾‹: è¼¸è¡€) ã¯ç¶­æŒ
    "è¼¸è¡€": [
        "ã€æº–å‚™ã€‘åŒæ„æ›¸ã®ç¢ºèªãŠã‚ˆã³æ‚£è€…ã¸ã®èª¬æ˜ã‚’è¡Œã„ã¾ã—ãŸã‹ï¼Ÿ",
        "ã€æº–å‚™ã€‘äº¤å·®é©åˆè©¦é¨“ã®çµæœã¨è¡€æ¶²è£½å‰¤ã€æŒ‡ç¤ºæ›¸ã®å†…å®¹ï¼ˆæ‚£è€…æ°åã€è¡€æ¶²å‹ã€æ”¾å°„ç·šç…§å°„æœ‰ç„¡ï¼‰ãŒä¸€è‡´ã—ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¾ã—ãŸã‹ï¼Ÿ",
        "ã€å®Ÿæ–½å‰ã€‘æ‚£è€…æ°åã€IDã€è¡€æ¶²å‹ã€è£½å‰¤ã®æœ‰åŠ¹æœŸé™ã€å¤–è¦³ï¼ˆå‡é›†ãƒ»å¤‰è‰²ãƒ»ç ´æï¼‰ã‚’åŒ»å¸«ãƒ»çœ‹è­·å¸«ã®2åã§å£°å‡ºã—ç¢ºèªã—ã¾ã—ãŸã‹ï¼Ÿ",
        "ã€å®Ÿæ–½ä¸­ã€‘æŠ•ä¸é–‹å§‹ç›´å‰ãŠã‚ˆã³é–‹å§‹å¾Œ5åˆ†ã€15åˆ†ã«ãƒã‚¤ã‚¿ãƒ«ã‚µã‚¤ãƒ³ã‚’æ¸¬å®šãƒ»è¦³å¯Ÿã—ã¾ã—ãŸã‹ï¼Ÿ",
        "ã€å®Ÿæ–½å¾Œã€‘å‰¯ä½œç”¨ã®æœ‰ç„¡ã‚’ç¢ºèªã—ã€ç©ºãƒãƒƒã‚°ã‚’æ‰€å®šã®æ–¹æ³•ã§ä¿ç®¡ãƒ»å»ƒæ£„ã—ã¾ã—ãŸã‹ï¼Ÿ"
    ],
    
    # è„³ç¥çµŒå¤–ç§‘ã§ç‰¹ã«é‡è¦ãªé …ç›®ã‚’å€‹åˆ¥ã«è¿½åŠ 
    "æ‚£è€…ç¢ºèªãƒ»æŒ‡å°": [
        "ã€ç¢ºèªã€‘æ‚£è€…ã®æ°åã¨IDã‚’ãƒªã‚¹ãƒˆãƒãƒ³ãƒ‰ã¨ç…§åˆã—ã€æœ¬äººã«åä¹—ã£ã¦ã‚‚ã‚‰ã„ç¢ºèªã—ã¾ã—ãŸã‹ï¼Ÿ",
        "ã€ç¢ºèªã€‘ã‚¢ãƒ¬ãƒ«ã‚®ãƒ¼æ­´ï¼ˆç‰¹ã«é€ å½±å‰¤ã‚¢ãƒ¬ãƒ«ã‚®ãƒ¼ï¼‰ã‚’å†ç¢ºèªã—ã€è¨˜éŒ²ã—ã¾ã—ãŸã‹ï¼Ÿ",
        "ã€æŒ‡å°ã€‘å‡¦ç½®ãƒ»æ¤œæŸ»å‰ã«ã€ä½“å‹•ãƒªã‚¹ã‚¯ã‚’è©•ä¾¡ã—ã€ä½“å‹•ã—ãªã„ã‚ˆã†å…·ä½“çš„ã‹ã¤ç°¡æ½”ã«èª¬æ˜ã—ã¾ã—ãŸã‹ï¼Ÿ",
        "ã€èª¬æ˜ã€‘æ‚£è€…ã¾ãŸã¯å®¶æ—ã«å¯¾ã—ã€ã“ã‚Œã‹ã‚‰è¡Œã†å‡¦ç½®ã‚„æ²»ç™‚å†…å®¹ã‚’èª¬æ˜ã—ã€åŒæ„ã‚’å¾—ã¾ã—ãŸã‹ï¼Ÿ",
    ],

    "ç‚¹æ»´ãƒ»è–¬å‰¤": [
        "ã€FIVE-RIGHTsã€‘åŒ»å¸«ãƒ»è–¬å‰¤å¸«ã®æŒ‡ç¤ºæ›¸ã«åŸºã¥ãã€æ­£ã—ã„è–¬å‰¤ã€é‡ã€æ™‚é–“ã€çµŒè·¯ã§ã‚ã‚‹ã“ã¨ã‚’ãƒ€ãƒ–ãƒ«ãƒã‚§ãƒƒã‚¯ã—ã¾ã—ãŸã‹ï¼Ÿ",
        "ã€æŠ—å‡å›ºè–¬ã€‘æ‰‹è¡“ã‚„ä¾µè¥²çš„å‡¦ç½®ã®å‰ã«ã€ä¼‘è–¬æŒ‡ç¤ºã¨æœ€çµ‚æŠ•ä¸æ™‚é–“ã‚’ç¢ºèªã—ã¾ã—ãŸã‹ï¼Ÿ",
        "ã€é«˜æµ¸é€åœ§è–¬ã€‘Mannitolãªã©ã®é«˜æµ¸é€åœ§è–¬ã«çµæ™¶åŒ–ã‚„æ²ˆæ®¿ç‰©ãŒãªã„ã‹ç¢ºèªã—ã€æŠ•ä¸é€Ÿåº¦ã¯æŒ‡ç¤ºé€šã‚Šã§ã™ã‹ï¼Ÿ",
        "ã€æŠ—ã¦ã‚“ã‹ã‚“è–¬ã€‘å‡¦æ–¹é–‹å§‹ãƒ»å¤‰æ›´æ™‚ã«ã€é©åˆ‡ãªè¡€ä¸­æ¿ƒåº¦æ¡è¡€ã‚ªãƒ¼ãƒ€ãƒ¼ãŒã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¾ã—ãŸã‹ï¼Ÿ",
        "ã€æŒç¶šç‚¹æ»´ã€‘ãƒãƒ³ãƒ—è¨­å®šï¼ˆè–¬å‰¤åã€å˜ä½ã€è¨­å®šé‡ï¼‰ã‚’2åã®ã‚¹ã‚¿ãƒƒãƒ•ã§å£°å‡ºã—ç¢ºèªã—ã¾ã—ãŸã‹ï¼Ÿ",
        "ã€ç®¡ç†ã€‘éº»è–¬ãƒ»å‘ç²¾ç¥è–¬ã¯æŠ•ä¸å‰å¾Œã®æ®‹è–¬ç¢ºèªã€è¨˜éŒ²ã€æ–½éŒ ä¿ç®¡ã‚’è¤‡æ•°äººã§è¡Œã„ã¾ã—ãŸã‹ï¼Ÿ",
    ],
    
    "ä¸­å¿ƒé™è„ˆã‚«ãƒ†ãƒ¼ãƒ†ãƒ«": [
        "ã€æº–å‚™ã€‘ã‚¨ã‚³ãƒ¼ã‚¬ã‚¤ãƒ‰ä¸‹ç©¿åˆºã®æº–å‚™ï¼ˆãƒ—ãƒ­ãƒ¼ãƒ–ã‚«ãƒãƒ¼ç­‰ï¼‰ã¯ã§ãã¦ã„ã¾ã™ã‹ï¼Ÿ",
        "ã€å®Ÿæ–½ä¸­ã€‘ã‚¬ã‚¤ãƒ‰ãƒ¯ã‚¤ãƒ¤ãƒ¼æŒ¿å…¥æ™‚ã€æŠµæŠ—ãŒãªã„ã“ã¨ã‚’ç¢ºèªã—ã¾ã—ãŸã‹ï¼Ÿï¼ˆç„¡ç†ãªæŒ¿å…¥ã¯ç¦æ­¢ï¼‰",
        "ã€å®Ÿæ–½ä¸­ã€‘å‹•è„ˆç©¿åˆºã®é™¤å¤–ï¼ˆçŸ­è»¸ãƒ»é•·è»¸åƒã§ã®ç¢ºèªã€åœ§æ³¢å½¢ãªã©ï¼‰ã‚’è¡Œã„ã¾ã—ãŸã‹ï¼Ÿ",
        "ã€å®Ÿæ–½å¾Œã€‘ã‚¬ã‚¤ãƒ‰ãƒ¯ã‚¤ãƒ¤ãƒ¼ãŒä½“å†…ã«æ®‹å­˜ã—ã¦ã„ãªã„ã“ã¨ã‚’æœ¬æ•°ç¢ºèªã—ã¾ã—ãŸã‹ï¼Ÿ",
        "ã€å®Ÿæ–½å¾Œã€‘ã‚«ãƒ†ãƒ¼ãƒ†ãƒ«å…ˆç«¯ä½ç½®ç¢ºèªã®ãŸã‚ã®Xç·šæ’®å½±ã‚ªãƒ¼ãƒ€ãƒ¼ã‚’è¡Œã„ã¾ã—ãŸã‹ï¼Ÿ",
        "ã€è¦³å¯Ÿã€‘åˆºå…¥éƒ¨ã®æ„ŸæŸ“å…†å€™ï¼ˆç™ºèµ¤ãƒ»è…«è„¹ï¼‰ã®æœ‰ç„¡ã‚’æ¯æ—¥ãƒã‚§ãƒƒã‚¯ã—ã¾ã—ãŸã‹ï¼Ÿ",
    ],
    
    "ãƒ‰ãƒ¬ãƒŠãƒ¼ã‚¸ç®¡ç†": [
        "ã€æŒ‡ç¤ºç¢ºèªã€‘ãƒ‰ãƒ¬ãƒŠãƒ¼ã‚¸ãƒãƒƒã‚°ã®**é«˜ã•ï¼ˆcmH2Oï¼‰**ã€ã‚¯ãƒ©ãƒ³ãƒ—ãƒ»é–‹æ”¾æŒ‡ç¤ºãŒæ˜ç¢ºã§ã™ã‹ï¼Ÿ",
        "ã€æ“ä½œç¢ºèªã€‘ä½“ä½å¤‰æ›ã‚„ç§»é€å‰å¾Œã§ã€æŒ‡ç¤ºã•ã‚ŒãŸãƒ‰ãƒ¬ãƒŠãƒ¼ã‚¸ãƒ©ã‚¤ãƒ³ã®ã‚¯ãƒ©ãƒ³ãƒ—æ“ä½œã‚’ç¢ºå®Ÿã«å®Ÿæ–½ã—ã¾ã—ãŸã‹ï¼Ÿ",
        "ã€æ’æ¶²è¦³å¯Ÿã€‘æ’æ¶²ã®**é‡ï¼ˆæ™‚é–“æ¯ï¼‰**ã€è‰²ã€æ··æ¿ã‚’è¨˜éŒ²ã—ã€æ€¥æ¿€ãªå¤‰åŒ–ã‚„ç•°å¸¸ãªé‡ã¯ã‚ã‚Šã¾ã›ã‚“ã‹ï¼Ÿ",
        "ã€é–‰å¡ç¢ºèªã€‘ãƒ©ã‚¤ãƒ³ã®å±ˆæ›²ã€é–‰å¡ãŒãªã„ã‹ç¢ºèªã—ã¾ã—ãŸã‹ï¼Ÿ ",
        "ã€åˆºå…¥éƒ¨ã€‘åˆºå…¥éƒ¨ã«æ„ŸæŸ“å…†å€™ãŒãªã„ã‹ç¢ºèªã—ã€ç„¡èŒæ“ä½œã§ãƒ‰ãƒ¬ãƒƒã‚·ãƒ³ã‚°æã‚’äº¤æ›ã—ã¾ã—ãŸã‹ï¼Ÿ",
    ],

    "è„³ç¥çµŒå¤–ç§‘ç®¡ç†": [
        "ã€æ„è­˜ãƒ¬ãƒ™ãƒ«ã€‘JCSã¾ãŸã¯GCSã«åŸºã¥ãã€æ­£ç¢ºã‹ã¤çµŒæ™‚çš„ã«æ„è­˜ãƒ¬ãƒ™ãƒ«ã‚’è©•ä¾¡ãƒ»è¨˜éŒ²ã—ã¾ã—ãŸã‹ï¼Ÿ",
        "ã€ç³å­”æ‰€è¦‹ã€‘ç³å­”å¾„ã¨å¯¾å…‰åå°„ã‚’å·¦å³ã§æ¯”è¼ƒã—ã€æ€¥æ¿€ãª**å·¦å³å·®ã®å‡ºç¾**ã‚„**æ•£ç³**ãŒãªã„ã‹ç¢ºèªã—ã¾ã—ãŸã‹ï¼Ÿ",
        "ã€éº»ç—ºè©•ä¾¡ã€‘é‹å‹•éº»ç—ºã‚„æ„Ÿè¦šéº»ç—ºã®æœ‰ç„¡ã€ãŠã‚ˆã³æ˜¨æ—¥ã‹ã‚‰ã®**é€²è¡Œãƒ»æ‚ªåŒ–**ãŒãªã„ã‹è©³ç´°ã«è©•ä¾¡ã—ã¾ã—ãŸã‹ï¼Ÿ",
        "ã€ãƒã‚¤ã‚¿ãƒ«ã€‘**ã‚¯ãƒƒã‚·ãƒ³ã‚°ç¾è±¡**ï¼ˆå¾è„ˆã€è¡€åœ§ä¸Šæ˜‡ï¼‰ãªã©ã®é ­è“‹å†…åœ§äº¢é€²ç—‡çŠ¶ã®ã‚µã‚¤ãƒ³ãŒãªã„ã‹ç¢ºèªã—ã¾ã—ãŸã‹ï¼Ÿ",
        "ã€ç·Šæ€¥ä½“åˆ¶ã€‘æ„è­˜éšœå®³ã‚„å‘¼å¸çŠ¶æ…‹ã®æ€¥å¤‰æ™‚ã€ã©ã®åŒ»å¸«ã«**ä½•åˆ†ä»¥å†…**ã«é€£çµ¡ã™ã‚‹ã‹ç¢ºèªã•ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ",
    ],

    # æ—¢å­˜ã®é …ç›®ï¼ˆæ¡è¡€ã€æ‰‹è¡“ã€æ°—ç®¡æŒ¿ç®¡ã€å†…è¦–é¡ï¼‰ã¯å¤‰æ›´ãªã—ã§ç¶­æŒ
    "æ¡è¡€": [
        "ã€æº–å‚™ã€‘æ¤œæŸ»æŒ‡ç¤ºæ›¸ã¨æ¡è¡€ç®¡ã®ãƒ©ãƒ™ãƒ«ï¼ˆæ°åã€IDã€æ¤œæŸ»é …ç›®ï¼‰ã‚’ç…§åˆã—ã¾ã—ãŸã‹ï¼Ÿ",
        "ã€å®Ÿæ–½å‰ã€‘æ‚£è€…æœ¬äººã«æ°åã‚’åä¹—ã£ã¦ã‚‚ã‚‰ã„ã€ãƒªã‚¹ãƒˆãƒãƒ³ãƒ‰ã¨ç…§åˆã—ã¾ã—ãŸã‹ï¼Ÿ",
        "ã€å®Ÿæ–½ä¸­ã€‘ç¥çµŒæå‚·äºˆé˜²ã®ãŸã‚ã€ç©¿åˆºæ™‚ã®æ¿€ç—›ã‚„ã—ã³ã‚Œã®æœ‰ç„¡ã‚’æ‚£è€…ã«ç¢ºèªã—ã¾ã—ãŸã‹ï¼Ÿ",
        "ã€å®Ÿæ–½ä¸­ã€‘é§†è¡€å¸¯ã¯1åˆ†ä»¥å†…ã«è§£é™¤ã—ã¾ã—ãŸã‹ï¼Ÿï¼ˆç‰¹ã«æŠœé‡å‰ã®è§£é™¤å¿˜ã‚Œã«æ³¨æ„ï¼‰",
        "ã€å®Ÿæ–½å¾Œã€‘æ­¢è¡€ç¢ºèªã‚’è¡Œã„ã€æ¡è¡€ç®¡ã®è»¢å€’æ··å’Œã‚’é©åˆ‡ã«è¡Œã„ã¾ã—ãŸã‹ï¼Ÿ"
    ],
    "æ‰‹è¡“": [
        "ã€Sign Inã€‘æ‚£è€…ç¢ºèªã€æ‰‹è¡“éƒ¨ä½ã€è¡“å¼ã®ç¢ºèªã€éº»é…”å™¨ãƒ»ãƒ¢ãƒ‹ã‚¿ãƒ¼ã®ãƒã‚§ãƒƒã‚¯ã¯å®Œäº†ã—ã¾ã—ãŸã‹ï¼Ÿ",
        "ã€Time Outã€‘åŸ·åˆ€ç›´å‰ã«å…¨ã‚¹ã‚¿ãƒƒãƒ•ã®æ‰‹ãŒæ­¢ã¾ã‚Šã€æ‚£è€…åãƒ»è¡“å¼ãƒ»éƒ¨ä½ãƒ»äºˆæƒ³ã•ã‚Œã‚‹å±é™ºæ“ä½œã‚’å…¨å“¡ã§å…±æœ‰ã—ã¾ã—ãŸã‹ï¼Ÿ",
        "ã€Time Outã€‘äºˆé˜²çš„æŠ—èŒè–¬ã®æŠ•ä¸ã¯åŸ·åˆ€60åˆ†ä»¥å†…ã«è¡Œã‚ã‚Œã¾ã—ãŸã‹ï¼Ÿ",
        "ã€Sign Outã€‘ã‚¬ãƒ¼ã‚¼ãƒ»å™¨æ¢°ãƒ»ç¸«åˆé‡ã®ã‚«ã‚¦ãƒ³ãƒˆæ•°ã¯ä¸€è‡´ã—ã¾ã—ãŸã‹ï¼Ÿ",
        "ã€Sign Outã€‘æ‘˜å‡ºæ¨™æœ¬ã®ãƒ©ãƒ™ãƒ«ï¼ˆæ‚£è€…åãƒ»æ¤œä½“åï¼‰ã¯æ­£ã—ã„ã§ã™ã‹ï¼Ÿ"
    ],
    "æ°—ç®¡æŒ¿ç®¡": [
        "ã€æº–å‚™ã€‘å–‰é ­é¡ã®ãƒ©ã‚¤ãƒˆç‚¹ç¯ã€ã‚«ãƒ•ã®ç ´æãŒãªã„ã‹ç¢ºèªã—ã¾ã—ãŸã‹ï¼Ÿ",
        "ã€æº–å‚™ã€‘å›°é›£æ°—é“ãŒäºˆæƒ³ã•ã‚Œã‚‹å ´åˆã€ãƒ“ãƒ‡ã‚ªå–‰é ­é¡ã‚„ãƒ–ã‚¸ãƒ¼ãªã©ã®ä»£æ›¿å™¨å…·ã‚’æº–å‚™ã—ã¾ã—ãŸã‹ï¼Ÿ",
        "ã€å®Ÿæ–½ä¸­ã€‘æŒ¿ç®¡å¾Œã€è´è¨ºï¼ˆ5ç‚¹è´è¨ºï¼‰ãŠã‚ˆã³ã‚«ãƒ—ãƒãƒ¡ãƒ¼ã‚¿ã§äºŒé…¸åŒ–ç‚­ç´ ã®æ³¢å½¢ã‚’ç¢ºèªã—ã¾ã—ãŸã‹ï¼Ÿ",
        "ã€å®Ÿæ–½å¾Œã€‘ãƒãƒ¥ãƒ¼ãƒ–ã®å›ºå®šä½ç½®ï¼ˆæ­¯åˆ—ã®cmï¼‰ã‚’è¨˜éŒ²ã—ã€ç¢ºå®Ÿã«å›ºå®šã—ã¾ã—ãŸã‹ï¼Ÿ",
        "ã€å®Ÿæ–½å¾Œã€‘èƒ¸éƒ¨Xç·šã§ãƒãƒ¥ãƒ¼ãƒ–å…ˆç«¯ä½ç½®ã‚’ç¢ºèªã—ã¾ã—ãŸã‹ï¼Ÿ"
    ],
    "å†…è¦–é¡": [
        "ã€æº–å‚™ã€‘å†…è¦–é¡æ´—æµ„æ¶ˆæ¯’å±¥æ­´ã‚’ç¢ºèªã—ã€ä½¿ç”¨æ©Ÿå™¨ã®å‹•ä½œç¢ºèªã‚’è¡Œã„ã¾ã—ãŸã‹ï¼Ÿ",
        "ã€å®Ÿæ–½å‰ã€‘æŠ—è¡€æ “è–¬ã®ä¼‘è–¬çŠ¶æ³ã€ã‚¢ãƒ¬ãƒ«ã‚®ãƒ¼æ­´ã€æ—¢å¾€æ­´ã‚’ç¢ºèªã—ã¾ã—ãŸã‹ï¼Ÿ",
        "ã€å®Ÿæ–½å‰ã€‘é®é™ã‚’è¡Œã†å ´åˆã€åŒæ„æ›¸ã®ç¢ºèªã¨è˜‡ç”Ÿç”¨å…·ï¼ˆé…¸ç´ ã€ã‚¢ãƒ³ãƒ“ãƒ¥ãƒ¼ç­‰ï¼‰ã®æº–å‚™ã¯ã§ãã¦ã„ã¾ã™ã‹ï¼Ÿ",
        "ã€å®Ÿæ–½ä¸­ã€‘æ‚£è€…ã®SpO2ã€å‘¼å¸çŠ¶æ…‹ã€è¡€åœ§ã®ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã‚’ç¶™ç¶šã—ã¦ã„ã¾ã™ã‹ï¼Ÿ",
        "ã€å®Ÿæ–½å¾Œã€‘è¦šé†’çŠ¶æ…‹ã‚’ç¢ºèªã—ã€é£²æ°´ãƒ»é£Ÿäº‹é–‹å§‹ã®æŒ‡ç¤ºã‚’æ˜ç¢ºã«ã—ã¾ã—ãŸã‹ï¼Ÿ"
    ]
}


# ==========================================
# 2. ãƒ­ã‚¸ãƒƒã‚¯é–¢æ•°ç¾¤
# ==========================================

def load_data() -> List[Dict]:
    """ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã‚€"""
    try:
        if os.path.exists(DATASET_PATH):
            with open(DATASET_PATH, "r", encoding="utf-8", errors='ignore') as f:
                return json.load(f)
        return []
    except Exception:
        if os.path.exists(DATASET_PATH):
            os.remove(DATASET_PATH)
        return []


def save_data(data: List[Dict]):
    """ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä¿å­˜ã™ã‚‹"""
    try:
        with open(DATASET_PATH, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    except Exception:
        pass


@st.cache_data
def load_checklists() -> Dict[str, str]:
    """ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€ (ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾è±¡)"""
    try:
        if not os.path.exists(CHECKLISTS_PATH):
            return {}
            
        with open(CHECKLISTS_PATH, "r", encoding="utf-8", errors='ignore') as f:
            return json.load(f)
    except Exception:
        return {}


def classify_procedure(text: str) -> str:
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å‡¦ç½®ãƒ»æ‰‹è¡“ã®ç¨®é¡ã‚’åˆ†é¡ã™ã‚‹"""
    if not text:
        return "ãã®ä»–"
    for proc, words in PROCEDURES.items():
        if any(w in text for w in words):
            return proc
    return "ãã®ä»–"


def is_likely_garbled(text: str) -> bool:
    """ãƒ†ã‚­ã‚¹ãƒˆãŒæ–‡å­—åŒ–ã‘ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒé«˜ã„ã‹åˆ¤å®šã™ã‚‹ã€‚"""
    if not text or len(text) < 5:
        return True

    total_len = len(text)
    japanese_pattern = re.compile(r'[\u4E00-\u9FFF\u3040-\u309F\u30A0-\u30FF\u0020-\u007E\uff00-\uffef]')
    valid_chars_count = len(japanese_pattern.findall(text))
    valid_ratio = valid_chars_count / total_len

    if valid_ratio < 0.1:
        return True
    if re.search(r'https?://', text) or re.search(r'[a-zA-Z]{3,4}://', text):
        return True

    return False


def extract_action_items(prevention_text: str) -> List[str]:
    """å…·ä½“çš„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã«åŸºã¥ã„ã¦ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆé …ç›®ã‚’æŠ½å‡ºã™ã‚‹"""
    actions = []
    sentences = re.split(r'[ã€‚\n]', prevention_text)

    for s in sentences:
        s = s.strip()
        if not s: continue
        if len(s) < 5 or len(s) > 100: continue
        if any(noise in s for noise in NOISE_KEYWORDS): continue

        if any(action in s for action in ACTION_KEYWORDS):
            cleaned_s = re.sub(r'[ã€ã€‚]$', '', s)
            cleaned_s = re.sub(r'^[-\d\.\sãƒ»]+', '', cleaned_s).strip()
            actions.append(cleaned_s)
    return actions


def extract_text_from_pdf(pdf_bytes: bytes) -> str:
    """PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã€å¼·åŠ›ãªæ–‡å­—åŒ–ã‘é™¤å»ã‚’è¡Œã†"""
    text = ""
    try:
        with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
            if pdf.pages:
                page = pdf.pages[0]
                text = page.extract_text(errors='ignore') or ""

        text_bytes = text.encode('utf-8', errors='ignore')
        text = text_bytes.decode('utf-8', errors='ignore')

        text = text.replace('\ufffd', '')
        text = re.sub(r'[\x00-\x1F\x7F]', '', text)
        text = text.replace(u'\xa0', u' ').replace('ã€€', ' ')

        text = re.sub(r'\s+', ' ', text).strip()

        for noise in NOISE_KEYWORDS:
            text = text.replace(noise, '')

        allowed_chars_regex = r'[^\u4E00-\u9FFF\u3040-\u309F\u30A0-\u30FF\u3000-\u303F\u0020-\u007E\uff10-\uff19\nã€ã€‚]'
        text = re.sub(allowed_chars_regex, '', text)

        return text
    except Exception:
        return ""


def parse_report_text(text: str, source_url: str) -> Dict[str, str]:
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰åŸå› ã¨å¯¾ç­–ã‚’åˆ‡ã‚Šå‡ºã™ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
    description = "æŠ½å‡ºä¸å¯"
    cause = ""
    prevention = ""

    if "æ¦‚è¦" in text:
        parts = text.split("æ¦‚è¦")
        if len(parts) > 1: description = parts[1][:200]

    if "åŸå› " in text:
        parts = text.split("åŸå› ")
        if len(parts) > 1: cause = parts[1][:200]

    if "å¯¾ç­–" in text:
        parts = text.split("å¯¾ç­–")
        if len(parts) > 1: prevention = parts[1][:300]
    elif "å†ç™ºé˜²æ­¢" in text:
        parts = text.split("å†ç™ºé˜²æ­¢")
        if len(parts) > 1: prevention = parts[1][:300]

    if len(description) < 10:
        description = text[:200]

    return {
        "source": source_url,
        "date": datetime.now().strftime("2025-12-01"),
        "department": "PDFè§£æ",
        "incident_type": classify_procedure(description),
        "description": description.replace('\n', ' ').strip(),
        "cause": cause.replace('\n', ' ').strip(),
        "prevention": prevention.replace('\n', ' ').strip(),
        "impact": "ä¸æ˜"
    }


def scrape_pdf_links() -> List[str]:
    """ã‚¿ãƒ¼ã‚²ãƒƒãƒˆURLã‹ã‚‰PDFãƒªãƒ³ã‚¯ã‚’åé›†"""
    pdf_links = set()
    base_url = "https://www.med-safe.jp"
    for url in TARGET_URLS:
        try:
            response = requests.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            for link in soup.find_all('a', href=True):
                href = link['href']
                if href.lower().endswith('.pdf'):
                    if href.startswith('/'):
                        abs_url = base_url + href
                    elif not href.startswith('http'):
                        abs_url = requests.compat.urljoin(url, href)
                    else:
                        abs_url = href
                    pdf_links.add(abs_url)
        except Exception:
            pass
    return list(pdf_links)


def scrape_and_update_dataset(limit_pdfs: int = 5) -> List[Dict]:
    """Webã‹ã‚‰PDFã‚’å–å¾—ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ›´æ–°"""
    pdf_urls = scrape_pdf_links()
    new_incidents: List[Dict] = []

    my_bar = st.progress(0)
    status_text = st.empty()

    target_pdfs = pdf_urls[:limit_pdfs]
    total = len(target_pdfs)

    for i, pdf_url in enumerate(target_pdfs):
        status_text.text(f"PDFè§£æä¸­ ({i + 1}/{total}): {pdf_url}")
        my_bar.progress((i + 1) / total)
        try:
            pdf_response = requests.get(pdf_url, timeout=30)
            raw_text = extract_text_from_pdf(pdf_response.content)
            if len(raw_text) > 50:
                record = parse_report_text(raw_text, pdf_url)
                new_incidents.append(record)
        except Exception:
            pass

    status_text.empty()
    my_bar.empty()

    current_data = load_data()
    combined_data = current_data + new_incidents
    save_data(combined_data)
    return combined_data


def run_checklist_generation(incidents: List[Dict]):
    """ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã¨æ¨™æº–é …ç›®ã‹ã‚‰ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆ"""
    causes = defaultdict(list)
    preventions_items = defaultdict(list)

    filtered_incidents = [item for item in incidents if not is_likely_garbled(item.get("description", ""))]

    for item in filtered_incidents:
        proc = classify_procedure(item.get("description", ""))
        cause = item.get("cause", "")
        prevention = item.get("prevention", "")
        if cause: causes[proc].append(cause.strip())
        if prevention:
            preventions_items[proc].extend(extract_action_items(prevention))

    checklists: Dict[str, str] = {}
    
    # PROCEDURESã®ã‚­ãƒ¼ã‚’å…¨ã¦å–å¾—ã—ã€ã‚½ãƒ¼ãƒˆã—ã¦ãƒ«ãƒ¼ãƒ—ã™ã‚‹
    all_procedures = sorted(list(STANDARD_CHECKLIST_ITEMS.keys()) + ["ãã®ä»–"])

    for proc in all_procedures:
        checklist: List[str] = []

        # 1. æ¨™æº–ãƒã‚§ãƒƒã‚¯é …ç›® (â˜…å¿…ãšè¡¨ç¤ºâ˜…)
        standard_items = STANDARD_CHECKLIST_ITEMS.get(proc, [])
        if standard_items:
            checklist.append(f"### ã€æ¨™æº–å®‰å…¨æ‰‹é †ï¼ˆ{proc}ï¼‰ã€‘")
            # ç¢ºå®Ÿãªç®‡æ¡æ›¸ãã®ãŸã‚ã®Markdownãƒªã‚¹ãƒˆè¨˜å·ã‚’è¿½åŠ 
            for p in standard_items: checklist.append(f"- âœ… {p}")

        # 2. äº‹ä¾‹ã‹ã‚‰ã®è¿½åŠ é …ç›®
        unique_actions = sorted(list(set(preventions_items[proc])))
        filtered_actions = [a for a in unique_actions if a not in standard_items]
        if filtered_actions:
            if checklist: checklist.append("")
            checklist.append("### ã€éå»ã®äº‹ä¾‹ã«å­¦ã¶è¿½åŠ ãƒã‚§ãƒƒã‚¯ã€‘")
            # ç¢ºå®Ÿãªç®‡æ¡æ›¸ãã®ãŸã‚ã®Markdownãƒªã‚¹ãƒˆè¨˜å·ã‚’è¿½åŠ 
            for p in filtered_actions: checklist.append(f"- â–¡ {p}")

        # 3. åŸå› 
        unique_causes = sorted(list(set(causes[proc])))
        if unique_causes:
            if checklist: checklist.append("")
            checklist.append("#### (å‚è€ƒ) éå»ã®ä¸»ãªåŸå› ")
            # ç¢ºå®Ÿãªç®‡æ¡æ›¸ãã®ãŸã‚ã®Markdownãƒªã‚¹ãƒˆè¨˜å·ã‚’è¿½åŠ 
            for c in unique_causes: checklist.append(f"- {c}")

        if checklist:
            checklists[proc] = "\n".join(checklist)

    # st.cache_dataã‚’ã‚¯ãƒªã‚¢ã—ã€æ–°ã—ã„ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆã‚’ä¿å­˜
    st.cache_data.clear()  
    with open(CHECKLISTS_PATH, "w", encoding="utf-8") as f:
        json.dump(checklists, f, ensure_ascii=False, indent=2)


def reset_system(limit_pdfs: int):
    """ã‚·ã‚¹ãƒ†ãƒ ã‚’ãƒªã‚»ãƒƒãƒˆã—å†æ§‹ç¯‰ã™ã‚‹"""
    if os.path.exists(DATASET_PATH): os.remove(DATASET_PATH)
    if os.path.exists(CHECKLISTS_PATH): os.remove(CHECKLISTS_PATH)

    incidents = scrape_and_update_dataset(limit_pdfs)
    run_checklist_generation(incidents)
    return incidents


# ==========================================
# 3. UI (Streamlit Pages)
# ==========================================

# â˜…â˜…â˜… page_viewer() é–¢æ•° (st.checkbox + st.session_stateã«ã‚ˆã‚‹çŠ¶æ…‹ä¿æŒ) â˜…â˜…â˜…
def page_viewer():
    st.title("ğŸ“‹ åŒ»ç™‚å®‰å…¨ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ")
    
    if not os.path.exists(CHECKLISTS_PATH):
        st.warning("âš ï¸ ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ãƒ‡ãƒ¼ã‚¿ç®¡ç†ãƒ»æ›´æ–°ãƒšãƒ¼ã‚¸ã§ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚")
        checklists = {}
    else:
        checklists = load_checklists()

    procedures = sorted(list(STANDARD_CHECKLIST_ITEMS.keys()) + ["ãã®ä»–"])
    
    default_index = 0
    if "è„³ç¥çµŒå¤–ç§‘ç®¡ç†" in procedures:
        default_index = procedures.index("è„³ç¥çµŒå¤–ç§‘ç®¡ç†")
    elif "è¼¸è¡€" in procedures:
        default_index = procedures.index("è¼¸è¡€")

    selected_proc = st.selectbox("å‡¦ç½®ã‚’é¸æŠã—ã¦ãã ã•ã„", procedures, index=default_index)

    st.markdown(f"## {selected_proc} ã®ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ")

    content = checklists.get(selected_proc)

    # --- ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹è¡¨ç¤ºã¨ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã«ã‚ˆã‚‹çŠ¶æ…‹ä¿æŒ ---

    if content:
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®åˆæœŸåŒ–
        if 'checklist_states' not in st.session_state:
            st.session_state['checklist_states'] = {}
            
        if selected_proc not in st.session_state['checklist_states']:
            st.session_state['checklist_states'][selected_proc] = {}

        # é …ç›®ã‚’è§£æã™ã‚‹ãŸã‚ã®å¤‰æ•°
        lines = content.split('\n')
        item_count = 0
        
        # ãƒã‚§ãƒƒã‚¯é …ç›®ã®ç·æ•°ã¨ãƒã‚§ãƒƒã‚¯æ¸ˆã¿ã®é …ç›®ã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        total_items = 0
        checked_items = 0
        
        # ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆã®è¡¨ç¤ºã¨å‡¦ç†
        for line in lines:
            line = line.strip()

            # 1. è¦‹å‡ºã—ã®å‡¦ç† (H3/H4)
            if line.startswith("### "):
                current_section = line.replace("### ", "--- \n**") + "**"
                st.markdown(current_section)
                continue
            if line.startswith("#### "):
                st.markdown(line)
                continue
                
            # 2. ãƒã‚§ãƒƒã‚¯é …ç›® (ãƒªã‚¹ãƒˆå½¢å¼: - âœ… ã¾ãŸã¯ - â–¡) ã®å‡¦ç†
            if line.startswith("- âœ… ") or line.startswith("- â–¡ "):
                # ãƒã‚§ãƒƒã‚¯é …ç›®ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
                item_text = line.replace("- âœ… ", "").replace("- â–¡ ", "").strip()
                
                # ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªã‚­ãƒ¼ã‚’ç”Ÿæˆ (å‡¦ç½®å_ã‚»ã‚¯ã‚·ãƒ§ãƒ³å_ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹)
                checkbox_key = f"chk_{selected_proc}_{item_count}"
                total_items += 1

                # st.checkboxã‚’ä½¿ç”¨ã—ã¦ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆã¨ã—ã¦è¡¨ç¤º
                # valueã¯ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã‹ã‚‰å–å¾—ã€‚å­˜åœ¨ã—ãªã„å ´åˆã¯False (æœªãƒã‚§ãƒƒã‚¯)
                is_checked = st.session_state['checklist_states'][selected_proc].get(checkbox_key, False)
                
                # ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã‚’è¡¨ç¤ºã€‚keyã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã§çŠ¶æ…‹ã‚’ä¿æŒ
                new_state = st.checkbox(item_text, value=is_checked, key=checkbox_key)
                
                # çŠ¶æ…‹ãŒå¤‰åŒ–ã—ãŸå ´åˆã€ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã‚’æ›´æ–° (ã“ã®ãƒ­ã‚¸ãƒƒã‚¯ã¯å†—é•·ã§ã™ãŒã€æ˜ç¤ºçš„ã«è¨˜è¿°ã™ã‚‹ã“ã¨ã§å‹•ä½œã‚’ä¿è¨¼)
                if new_state != is_checked:
                    st.session_state['checklist_states'][selected_proc][checkbox_key] = new_state
                    
                if new_state:
                    checked_items += 1
                    
                item_count += 1
            
            # 3. ãã®ä»–ã®è¡Œï¼ˆåŸå› ã®ãƒªã‚¹ãƒˆé …ç›®ãªã©ï¼‰ã®å‡¦ç†
            elif line:
                st.markdown(line)
        
        # é€²æ—ãƒãƒ¼ã®è¡¨ç¤º
        if total_items > 0:
            progress_ratio = checked_items / total_items
            st.progress(progress_ratio, text=f"**é€²æ—çŠ¶æ³: {checked_items} / {total_items} é …ç›®å®Œäº†**")
        else:
            st.info("ã“ã®ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆã«ã¯ãƒã‚§ãƒƒã‚¯é …ç›®ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

        # å‡¦ç½®ãŒå®Œäº†ã—ãŸã‚‰ãƒã‚§ãƒƒã‚¯çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆã™ã‚‹ãƒœã‚¿ãƒ³
        if st.button("ã“ã®å‡¦ç½®ã®ãƒã‚§ãƒƒã‚¯çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ"):
            if selected_proc in st.session_state['checklist_states']:
                st.session_state['checklist_states'][selected_proc] = {}
                st.rerun() # ãƒªã‚»ãƒƒãƒˆå¾Œã€ç”»é¢ã‚’å†æç”»ã—ã¦ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã‚’æœªãƒã‚§ãƒƒã‚¯ã«ã™ã‚‹
            
    # --- ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹è¡¨ç¤ºã¨ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã«ã‚ˆã‚‹çŠ¶æ…‹ä¿æŒã®çµ‚ã‚ã‚Š ---
    
    else:
        # ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã®æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯
        standard = STANDARD_CHECKLIST_ITEMS.get(selected_proc)
        if standard:
            st.warning("âš ï¸ æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚æ¨™æº–æ‰‹é †ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚")
            dummy_content = f"### ã€æ¨™æº–å®‰å…¨æ‰‹é †ï¼ˆ{selected_proc}ï¼‰ã€‘\n" + "\n".join([f"- âœ… {p}" for p in standard])
            # ã“ã®ãƒ€ãƒŸãƒ¼ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚‚st.checkboxã¨ã—ã¦å‡¦ç†ã™ã‚‹æ–¹ãŒè¦ªåˆ‡ã§ã™ãŒã€ä»Šå›ã¯ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã®æš«å®šè¡¨ç¤ºã¨ã—ã¦markdownã®ã¾ã¾ã«ã—ã¾ã™ã€‚
            st.markdown(dummy_content)
        else:
            st.info("æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ã€Œãƒ‡ãƒ¼ã‚¿ç®¡ç†ãƒ»æ›´æ–°ã€ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹ã‹ã€PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")


def page_manager():
    st.title("âš™ï¸ ãƒ‡ãƒ¼ã‚¿ç®¡ç†ãƒ»æ›´æ–°")

    st.subheader("1. ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–ï¼ˆWebãƒ‡ãƒ¼ã‚¿å–å¾—ï¼‰")
    st.caption("Webä¸Šã®ãƒ’ãƒ¤ãƒªãƒ»ãƒãƒƒãƒˆå ±å‘Šæ›¸(PDF)ã‚’è§£æã—ã€ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆã‚’è‡ªå‹•ç”Ÿæˆã—ã¾ã™ã€‚")

    limit = st.number_input("è§£æã™ã‚‹PDFæ•° (å¤šã„ã¨æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™)", 1, 50, 5)

    if st.button("ğŸ”„ ã‚·ã‚¹ãƒ†ãƒ ã‚’å®Œå…¨ãƒªã‚»ãƒƒãƒˆã—ã¦å†æ§‹ç¯‰"):
        with st.spinner("ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ã€Webã‹ã‚‰å†å–å¾—ä¸­..."):
            st.cache_data.clear()
            incidents = reset_system(limit)

        clean_incidents_count = len([i for i in incidents if not is_likely_garbled(i.get("description", ""))])
        st.success(
            f"å®Œäº†ã—ã¾ã—ãŸã€‚å…¨ {len(incidents)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€ã†ã¡ {clean_incidents_count} ä»¶ãŒæœ‰åŠ¹ãªäº‹ä¾‹ã¨ã—ã¦è§£æã•ã‚Œã¾ã—ãŸã€‚")
        st.info("å·¦ã®ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‹ã‚‰ã€Œãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆãƒ“ãƒ¥ãƒ¼ã‚¢ã€ã¸ç§»å‹•ã—ã¦ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

    st.markdown("---")

    st.subheader("2. PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«è¿½åŠ ")
    st.caption("ãŠæ‰‹å…ƒã®ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå ±å‘Šæ›¸PDFã‚’ç›´æ¥è§£æã—ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«è¿½åŠ ã—ã¾ã™ã€‚")

    uploaded_file = st.file_uploader("ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå ±å‘Šæ›¸ (PDF)", type="pdf")

    if uploaded_file is not None:
        if st.button("ğŸ“„ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸPDFã‚’è§£æ"):
            with st.spinner("PDFã‚’è§£æä¸­..."):
                try:
                    pdf_bytes = uploaded_file.read()
                    raw_text = extract_text_from_pdf(pdf_bytes)

                    if len(raw_text) > 100 and not is_likely_garbled(raw_text):
                        record = parse_report_text(raw_text, f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«: {uploaded_file.name}")

                        current = load_data()
                        current.append(record)
                        save_data(current)
                        run_checklist_generation(current)

                        st.success(f"PDFãƒ•ã‚¡ã‚¤ãƒ«ã€Œ{uploaded_file.name}ã€ã®è§£æã«æˆåŠŸã—ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒæ›´æ–°ã•ã‚Œã¾ã—ãŸã€‚")
                        st.markdown(f"**è§£æçµæœæ¦‚è¦:** {record['description']}")
                        st.info("å·¦ã®ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‹ã‚‰ã€Œãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆãƒ“ãƒ¥ãƒ¼ã‚¢ã€ã¸ç§»å‹•ã—ã¦ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                    else:
                        st.error("ã‚¨ãƒ©ãƒ¼: PDFã‹ã‚‰æœ‰åŠ¹ãªæ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ•ã‚¡ã‚¤ãƒ«ãŒæš—å·åŒ–ã•ã‚Œã¦ã„ã‚‹ã‹ã€æ–‡å­—åŒ–ã‘ãŒæ¿€ã—ã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
                except Exception as e:
                    st.error(f"è§£æä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

    st.markdown("---")

    st.subheader("3. æ‰‹å‹•ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆè¿½åŠ ")
    st.caption("é™¢å†…ã§ç™ºç”Ÿã—ãŸç‹¬è‡ªã®äº‹ä¾‹ã‚’æ‰‹å‹•ã§å…¥åŠ›ã—ã€ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆã‚’ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã—ã¾ã™ã€‚")

    with st.form("manual_add"):
        # STANDARD_CHECKLIST_ITEMSã®ã‚­ãƒ¼ã‚’å‡¦ç½®ç¨®é¡ã¨ã—ã¦ä½¿ç”¨
        m_proc_options = sorted(list(STANDARD_CHECKLIST_ITEMS.keys()) + ["ãã®ä»–"])
        m_proc = st.selectbox("å‡¦ç½®ç¨®é¡", m_proc_options)
        m_desc = st.text_area("ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆæ¦‚è¦", placeholder="ä¾‹ï¼šè¼¸è¡€æ™‚ã«æ‚£è€…IDã®ç¢ºèªã‚’çœç•¥ã—ãã†ã«ãªã£ãŸ")
        m_cause = st.text_area("åŸå› ", placeholder="ä¾‹ï¼šæ€¥ã„ã§ã„ãŸãŸã‚ã€ãƒ€ãƒ–ãƒ«ãƒã‚§ãƒƒã‚¯ãŒå½¢å¼çš„ã«ãªã£ã¦ã„ãŸ")
        m_prev = st.text_area("å†ç™ºé˜²æ­¢ç­–ãƒ»æ•™è¨“", placeholder="ä¾‹ï¼šæŒ‡å·®ã—å‘¼ç§°ã‚’å¿…é ˆã¨ã™ã‚‹")

        if st.form_submit_button("ãƒªã‚¹ãƒˆã«è¿½åŠ "):
            new_record = {
                "incident_type": m_proc,
                "description": m_desc,
                "cause": m_cause,
                "prevention": m_prev,
                "source": "æ‰‹å‹•å…¥åŠ›",
                "date": datetime.now().strftime("2025-12-01")
            }
            current = load_data()
            current.append(new_record)
            save_data(current)
            run_checklist_generation(current)
            st.success("è¿½åŠ ã—ã¾ã—ãŸï¼ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆãŒæ›´æ–°ã•ã‚Œã¾ã—ãŸã€‚")

    st.markdown("---")
    st.subheader("ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¦‚è¦ (æœ€æ–°10ä»¶)")
    incidents = load_data()

    clean_incidents = [i for i in incidents if not is_likely_garbled(i.get("description", ""))]

    if clean_incidents:
        df = pd.DataFrame([
            {"ç¨®åˆ¥": i.get("incident_type"),
             "æ¦‚è¦": i.get("description", "").replace('\n', ' ')[:40] + "...",
             "åŸå› ": i.get("cause", "").replace('\n', ' ')[:40] + "..."
             }
            for i in clean_incidents[-10:]
        ])
        st.caption(f"å…¨ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(incidents)}ä»¶ (ã†ã¡ã€æ–‡å­—åŒ–ã‘ã‚’é™¤å¤–ã—ãŸæœ‰åŠ¹ä»¶æ•°: {len(clean_incidents)}ä»¶)")
        st.table(df)
    else:
        st.write("æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚PDFã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã¾ãŸã¯æ‰‹å‹•å…¥åŠ›ã‚’è©¦ã—ã¦ãã ã•ã„ã€‚")


# ==========================================
# 4. ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œéƒ¨
# ==========================================
def main():
    st.set_page_config(page_title="åŒ»ç™‚å®‰å…¨AI", layout="wide")
    
    # â˜…â˜…â˜… æœ€çµ‚å¼·åˆ¶ãƒªã‚»ãƒƒãƒˆãƒ­ã‚¸ãƒƒã‚¯ â˜…â˜…â˜…
    if os.path.exists(CHECKLISTS_PATH) and not st.session_state.get('initial_load_done', False):
        try:
            st.session_state['initial_load_done'] = True
            
            with open(CHECKLISTS_PATH, 'r', encoding='utf-8') as f:
                content = json.load(f)
                # ãƒ‡ãƒ¼ã‚¿ã®ã‚µã‚¤ã‚ºãŒéå¸¸ã«å°ã•ã„å ´åˆã¯ã€å¤ã„ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚å†æ§‹ç¯‰
                if len(content.get('è¼¸è¡€', '')) < 100:
                    st.warning("ğŸ”„ å¤ã„ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚æœ€æ–°ã®ã‚³ãƒ¼ãƒ‰ã§ãƒªã‚¹ãƒˆã‚’å†ç”Ÿæˆã—ã¾ã™ã€‚")
                    if os.path.exists(DATASET_PATH):  
                        incidents = load_data()
                        run_checklist_generation(incidents)
                    else:
                        run_checklist_generation([])

        except (json.JSONDecodeError, FileNotFoundError):
            if os.path.exists(DATASET_PATH):  
                incidents = load_data()
                run_checklist_generation(incidents)
            else:
                run_checklist_generation([])

    st.sidebar.title("ãƒ¡ãƒ‹ãƒ¥ãƒ¼")
    page = st.sidebar.radio("æ©Ÿèƒ½é¸æŠ", ["ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆãƒ“ãƒ¥ãƒ¼ã‚¢", "ãƒ‡ãƒ¼ã‚¿ç®¡ç†ãƒ»æ›´æ–°"])

    if page == "ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆãƒ“ãƒ¥ãƒ¼ã‚¢":
        page_viewer()
    elif page == "ãƒ‡ãƒ¼ã‚¿ç®¡ç†ãƒ»æ›´æ–°":
        page_manager()


if __name__ == "__main__":
    main()
